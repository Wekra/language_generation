{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprachgenerierung\n",
    "\n",
    "Auch in dieser Woche beschäftigen wir uns mit Twitter-Daten. Diesmal geht es jedoch nicht um Sentiment Analysis, sondern wir wollen ein Modell trainieren, das es uns erlaubt, Tweets in einem bestimmten Stil zu generieren.\n",
    "\n",
    "Als Datengrundlage dienen uns Daten von http://www.trumptwitterarchive.com. Brendan Brown, der Betreiber der Seite hat sämtliche Tweets von Donald Trump seit Mai 2009 zusammengetragen. Da wir die Sprache des US-Präsidenten modellieren wollen, verwenden wir nur dessen eigene und keine Retweets.\n",
    "\n",
    "Unser Modell wird auf der Ebene von Einzelzeichen arbeiten, zunächst wollen wir uns aber auf einer höheren Ebene einen Überblick über den Datensatz verschaffen.\n",
    "\n",
    "## 1. Aufgabe: Überblick über den Datensatz\n",
    "### 1.1 Datensatz einlesen\n",
    "Lest den in der Datei ```all_tweets.json``` enthaltenen Datensatz in einen Pandas-Dataframe mit folgenden Spalten ein: ```created_at```, ```id```, ```text```. Die übrigen im JSON enthaltenen Felder können ignoriert werden.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "input_file = 'all_tweets.json'\n",
    "tweets = pd.read_json(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.drop(['favorite_count', 'is_retweet', 'retweet_count', 'source'], axis=1)\n",
    "\n",
    "# tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Jahr hinzufügen\n",
    "Für unsere Auswertungen interessiert uns nur das Jahr, in dem der Tweet verfasst wurde, nicht das genaue Datum. Wir fügen daher dem Dataframe eine zusätzliche Spalte ```year``` hinzu und verwenden die Pandas-Funktion ```DatetimeIndex```, um aus dem String einen DatetimeIndex zu machen, auf dessen einzelne Felder (```year```, ```month```, ```day```, ...) dann mittels Punktoperator zugegriffen werden kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['year'] = pd.DatetimeIndex(tweets['created_at']).year\n",
    "# tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Textlänge analysieren\n",
    "Naturgemäß gibt es bei Tweets nur eine begrenzte Varianz, was die Textlänge angeht. Wir wollen uns dennoch anschauen, wie sich die Textlänge im Laufe der Jahre entwickelt hat.\n",
    "Dazu fügen wir unserem Dataframe zunächst eine Spalte ```text_length``` hinzu, in der wir festhalten, welche Länge der jeweilige Tweet-Text hat.\n",
    "\n",
    "**Hinweis**\n",
    "Mittels ```apply``` lassen sich Funktionen auf Spalten des Dataframes mappen: ```df['new'] = df['old'].apply(lambda x : fancy_stuff(x))```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['text_length'] = tweets['text'].apply(lambda x: len(x))\n",
    "# tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für einen groben Überblick schauen wir uns einige Kennzahlen zur Textlänge an. Dazu gruppieren wir nach ```year```und nutzen dann die ```describe```-Methode des Dataframes, wobei wir nur Spalten vom Typ ```numpy.number``` betrachten und daher der ```describe```-Methode eine entsprechende ```include```-Liste mitgeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">id_str</th>\n",
       "      <th colspan=\"8\" halign=\"left\">text_length</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>56.0</td>\n",
       "      <td>2.896500e+09</td>\n",
       "      <td>1.386292e+09</td>\n",
       "      <td>1.698309e+09</td>\n",
       "      <td>1.863037e+09</td>\n",
       "      <td>2.280281e+09</td>\n",
       "      <td>3.462656e+09</td>\n",
       "      <td>6.971080e+09</td>\n",
       "      <td>56.0</td>\n",
       "      <td>112.214286</td>\n",
       "      <td>20.056932</td>\n",
       "      <td>62.0</td>\n",
       "      <td>103.75</td>\n",
       "      <td>115.0</td>\n",
       "      <td>126.25</td>\n",
       "      <td>140.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>142.0</td>\n",
       "      <td>1.440030e+15</td>\n",
       "      <td>4.090839e+15</td>\n",
       "      <td>7.677152e+09</td>\n",
       "      <td>1.318076e+10</td>\n",
       "      <td>1.753370e+10</td>\n",
       "      <td>2.455505e+10</td>\n",
       "      <td>2.059549e+16</td>\n",
       "      <td>142.0</td>\n",
       "      <td>122.528169</td>\n",
       "      <td>21.394536</td>\n",
       "      <td>45.0</td>\n",
       "      <td>111.00</td>\n",
       "      <td>132.5</td>\n",
       "      <td>138.00</td>\n",
       "      <td>140.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>774.0</td>\n",
       "      <td>1.163851e+17</td>\n",
       "      <td>2.985790e+16</td>\n",
       "      <td>2.521253e+16</td>\n",
       "      <td>1.009352e+17</td>\n",
       "      <td>1.214507e+17</td>\n",
       "      <td>1.416105e+17</td>\n",
       "      <td>1.528272e+17</td>\n",
       "      <td>774.0</td>\n",
       "      <td>107.739018</td>\n",
       "      <td>27.061936</td>\n",
       "      <td>38.0</td>\n",
       "      <td>86.00</td>\n",
       "      <td>114.0</td>\n",
       "      <td>133.00</td>\n",
       "      <td>140.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>3530.0</td>\n",
       "      <td>2.338664e+17</td>\n",
       "      <td>3.690381e+16</td>\n",
       "      <td>1.542710e+17</td>\n",
       "      <td>2.047411e+17</td>\n",
       "      <td>2.456090e+17</td>\n",
       "      <td>2.636903e+17</td>\n",
       "      <td>2.847717e+17</td>\n",
       "      <td>3530.0</td>\n",
       "      <td>106.975921</td>\n",
       "      <td>32.147123</td>\n",
       "      <td>12.0</td>\n",
       "      <td>87.00</td>\n",
       "      <td>117.0</td>\n",
       "      <td>135.00</td>\n",
       "      <td>148.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>5775.0</td>\n",
       "      <td>3.391614e+17</td>\n",
       "      <td>3.727767e+16</td>\n",
       "      <td>2.861273e+17</td>\n",
       "      <td>3.063865e+17</td>\n",
       "      <td>3.300694e+17</td>\n",
       "      <td>3.699051e+17</td>\n",
       "      <td>4.181450e+17</td>\n",
       "      <td>5775.0</td>\n",
       "      <td>86.123983</td>\n",
       "      <td>43.081747</td>\n",
       "      <td>9.0</td>\n",
       "      <td>41.00</td>\n",
       "      <td>94.0</td>\n",
       "      <td>128.00</td>\n",
       "      <td>152.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>2433.0</td>\n",
       "      <td>4.880322e+17</td>\n",
       "      <td>3.704741e+16</td>\n",
       "      <td>4.183651e+17</td>\n",
       "      <td>4.573088e+17</td>\n",
       "      <td>4.893819e+17</td>\n",
       "      <td>5.208777e+17</td>\n",
       "      <td>5.503998e+17</td>\n",
       "      <td>2433.0</td>\n",
       "      <td>114.132758</td>\n",
       "      <td>27.615667</td>\n",
       "      <td>21.0</td>\n",
       "      <td>98.00</td>\n",
       "      <td>124.0</td>\n",
       "      <td>136.00</td>\n",
       "      <td>148.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>3057.0</td>\n",
       "      <td>6.229778e+17</td>\n",
       "      <td>3.891979e+16</td>\n",
       "      <td>5.505476e+17</td>\n",
       "      <td>5.919861e+17</td>\n",
       "      <td>6.235615e+17</td>\n",
       "      <td>6.582454e+17</td>\n",
       "      <td>6.827032e+17</td>\n",
       "      <td>3057.0</td>\n",
       "      <td>109.009486</td>\n",
       "      <td>31.873999</td>\n",
       "      <td>15.0</td>\n",
       "      <td>87.00</td>\n",
       "      <td>120.0</td>\n",
       "      <td>137.00</td>\n",
       "      <td>155.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>3465.0</td>\n",
       "      <td>7.438493e+17</td>\n",
       "      <td>3.707584e+16</td>\n",
       "      <td>6.827240e+17</td>\n",
       "      <td>7.087148e+17</td>\n",
       "      <td>7.452963e+17</td>\n",
       "      <td>7.779584e+17</td>\n",
       "      <td>8.152709e+17</td>\n",
       "      <td>3465.0</td>\n",
       "      <td>113.363925</td>\n",
       "      <td>30.697098</td>\n",
       "      <td>14.0</td>\n",
       "      <td>93.00</td>\n",
       "      <td>127.0</td>\n",
       "      <td>138.00</td>\n",
       "      <td>148.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>2284.0</td>\n",
       "      <td>8.865922e+17</td>\n",
       "      <td>3.796267e+16</td>\n",
       "      <td>8.154223e+17</td>\n",
       "      <td>8.560855e+17</td>\n",
       "      <td>8.906965e+17</td>\n",
       "      <td>9.189161e+17</td>\n",
       "      <td>9.476141e+17</td>\n",
       "      <td>2284.0</td>\n",
       "      <td>133.917688</td>\n",
       "      <td>46.279751</td>\n",
       "      <td>2.0</td>\n",
       "      <td>119.00</td>\n",
       "      <td>138.0</td>\n",
       "      <td>143.00</td>\n",
       "      <td>320.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>3104.0</td>\n",
       "      <td>1.020171e+18</td>\n",
       "      <td>3.587214e+16</td>\n",
       "      <td>9.478026e+17</td>\n",
       "      <td>9.928010e+17</td>\n",
       "      <td>1.023553e+18</td>\n",
       "      <td>1.051105e+18</td>\n",
       "      <td>1.079888e+18</td>\n",
       "      <td>3104.0</td>\n",
       "      <td>198.084407</td>\n",
       "      <td>83.564703</td>\n",
       "      <td>8.0</td>\n",
       "      <td>131.00</td>\n",
       "      <td>221.0</td>\n",
       "      <td>277.00</td>\n",
       "      <td>315.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>1075.0</td>\n",
       "      <td>1.102251e+18</td>\n",
       "      <td>1.272214e+16</td>\n",
       "      <td>1.079900e+18</td>\n",
       "      <td>1.090609e+18</td>\n",
       "      <td>1.103036e+18</td>\n",
       "      <td>1.113900e+18</td>\n",
       "      <td>1.122486e+18</td>\n",
       "      <td>1075.0</td>\n",
       "      <td>183.999070</td>\n",
       "      <td>92.820761</td>\n",
       "      <td>5.0</td>\n",
       "      <td>102.50</td>\n",
       "      <td>205.0</td>\n",
       "      <td>276.00</td>\n",
       "      <td>302.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id_str                                                          \\\n",
       "       count          mean           std           min           25%   \n",
       "year                                                                   \n",
       "2009    56.0  2.896500e+09  1.386292e+09  1.698309e+09  1.863037e+09   \n",
       "2010   142.0  1.440030e+15  4.090839e+15  7.677152e+09  1.318076e+10   \n",
       "2011   774.0  1.163851e+17  2.985790e+16  2.521253e+16  1.009352e+17   \n",
       "2012  3530.0  2.338664e+17  3.690381e+16  1.542710e+17  2.047411e+17   \n",
       "2013  5775.0  3.391614e+17  3.727767e+16  2.861273e+17  3.063865e+17   \n",
       "2014  2433.0  4.880322e+17  3.704741e+16  4.183651e+17  4.573088e+17   \n",
       "2015  3057.0  6.229778e+17  3.891979e+16  5.505476e+17  5.919861e+17   \n",
       "2016  3465.0  7.438493e+17  3.707584e+16  6.827240e+17  7.087148e+17   \n",
       "2017  2284.0  8.865922e+17  3.796267e+16  8.154223e+17  8.560855e+17   \n",
       "2018  3104.0  1.020171e+18  3.587214e+16  9.478026e+17  9.928010e+17   \n",
       "2019  1075.0  1.102251e+18  1.272214e+16  1.079900e+18  1.090609e+18   \n",
       "\n",
       "                                               text_length              \\\n",
       "               50%           75%           max       count        mean   \n",
       "year                                                                     \n",
       "2009  2.280281e+09  3.462656e+09  6.971080e+09        56.0  112.214286   \n",
       "2010  1.753370e+10  2.455505e+10  2.059549e+16       142.0  122.528169   \n",
       "2011  1.214507e+17  1.416105e+17  1.528272e+17       774.0  107.739018   \n",
       "2012  2.456090e+17  2.636903e+17  2.847717e+17      3530.0  106.975921   \n",
       "2013  3.300694e+17  3.699051e+17  4.181450e+17      5775.0   86.123983   \n",
       "2014  4.893819e+17  5.208777e+17  5.503998e+17      2433.0  114.132758   \n",
       "2015  6.235615e+17  6.582454e+17  6.827032e+17      3057.0  109.009486   \n",
       "2016  7.452963e+17  7.779584e+17  8.152709e+17      3465.0  113.363925   \n",
       "2017  8.906965e+17  9.189161e+17  9.476141e+17      2284.0  133.917688   \n",
       "2018  1.023553e+18  1.051105e+18  1.079888e+18      3104.0  198.084407   \n",
       "2019  1.103036e+18  1.113900e+18  1.122486e+18      1075.0  183.999070   \n",
       "\n",
       "                                                     \n",
       "            std   min     25%    50%     75%    max  \n",
       "year                                                 \n",
       "2009  20.056932  62.0  103.75  115.0  126.25  140.0  \n",
       "2010  21.394536  45.0  111.00  132.5  138.00  140.0  \n",
       "2011  27.061936  38.0   86.00  114.0  133.00  140.0  \n",
       "2012  32.147123  12.0   87.00  117.0  135.00  148.0  \n",
       "2013  43.081747   9.0   41.00   94.0  128.00  152.0  \n",
       "2014  27.615667  21.0   98.00  124.0  136.00  148.0  \n",
       "2015  31.873999  15.0   87.00  120.0  137.00  155.0  \n",
       "2016  30.697098  14.0   93.00  127.0  138.00  148.0  \n",
       "2017  46.279751   2.0  119.00  138.0  143.00  320.0  \n",
       "2018  83.564703   8.0  131.00  221.0  277.00  315.0  \n",
       "2019  92.820761   5.0  102.50  205.0  276.00  302.0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "tweets_grouped = tweets.groupby(['year']).describe(include=[np.number])\n",
    "\n",
    "tweets_grouped\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Top-Hashtags und -Mentions\n",
    "Nachdem wir uns mit der Länge der Texte beschäftigt haben, wollen wir nun herausfinden, wen Donald Trump in seinen Tweets erwähnt und welche Themen er (hash)taggt. Dabei interessiert uns die Entwicklung über die Jahre.\n",
    "\n",
    "Wir beginnen mit den Hashtags und verwenden zunächst einen kleinen Trick, um ein Dictionary zu erstellen, das für jedes Jahr einen \"Sub-Dataframe\" enthält. \n",
    "\n",
    "Aus diesen Dataframes extrahieren wir dann pro Jahr alle Tweettexte in Form eines einzelnen Strings. Dazu konkatenieren wir die ```text```-Felder der Dataframes per ```' '.join(frame['text']) for frame in ...```\n",
    "\n",
    "Damit haben wir ein Dictionary, das pro Jahr alle konkatenierten Tweet-Texte enthält, aus denen wir dann die Hashtags extrahieren können. \n",
    "Hierbei machen wir uns noch keine allzu großen Gedanken über Normalisierung, sondern zerlegen die langen Texte einfach per ```split()``` in einzelne Tokens, aus denen wir dann die Hashtags herausfiltern. \n",
    "Um die Top-Hashtags in Erfahrung zu bringen, verwenden wir wieder ```Counter```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "tweets_by_year = dict(list(tweets.groupby(['year'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_in_year_dict = {}\n",
    "\n",
    "for year in tweets_by_year:\n",
    "    tweets_in_year = ' '.join(tweets_by_year[year]['text'])\n",
    "    tweets_in_year_dict[year] = tweets_in_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags_in_year_dict = {}\n",
    "\n",
    "for year in tweets_in_year_dict:\n",
    "    tl = tweets_in_year_dict[year].split()\n",
    "    hashtag_list = list(filter(lambda x: '#' in x, tl))\n",
    "    hashtags_in_year_dict[year] = hashtag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "top_hashtags_in_year_dict = {}\n",
    "\n",
    "for year in hashtags_in_year_dict:\n",
    "    hashtag_count = Counter(hashtags_in_year_dict[year])\n",
    "    top_hashtags_in_year_dict[year] = hashtag_count.most_common(20)\n",
    "#     print(\"\\n\\n\",top_hashtags_in_year_dict[year])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als nächstes interessieren uns die Mentions. Wir können hier analog zu den Hashtags vorgehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_mentions_per_year = {}\n",
    "\n",
    "for year in tweets_in_year_dict:\n",
    "    tl = tweets_in_year_dict[year].split()\n",
    "    mention_list = list(filter(lambda x: '@' in x, tl))\n",
    "    mention_count = Counter(mention_list)\n",
    "    top_mentions_per_year[year] = mention_count.most_common(20)\n",
    "\n",
    "# top_mentions_per_year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Vokabular\n",
    "Bevor wir endgültig auf die Ebene der Einzelzeichen herabsteigen, wollen wir uns das von Trump verwendete Vokabular genauer ansehen und dabei auch herausfinden, ob weitere Vorverarbeitungsschritte nötig sind.\n",
    "\n",
    "Dazu erstellen wir uns zunächst eine Liste aller Tokens, die in den Dokumenten vorkommen. Wir gruppieren nicht mehr per Jahr, sondern gehen ganz simpel vor und konkatenieren alle Tweet-Texte in einen langen String, den wir dann in einzelne Terme splitten.\n",
    "\n",
    "Gebt die 20 häufigsten Terme aus. Was fällt auf (insbesondere bei Betrachtung des hinteren Endes der Liste)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 17252), ('to', 11653), ('and', 9303), ('of', 8246), ('a', 7975), ('is', 7535), ('in', 6777), ('for', 5067), ('I', 4861), ('on', 4842), ('be', 4023), ('will', 3761), ('that', 3217), ('are', 3177), ('with', 2980), ('you', 2963), ('at', 2800), ('&amp;', 2764), ('The', 2759), ('have', 2546)]\n",
      "['Will', 'be', 'interviewed', 'by', '@MariaBartiromo', 'on', '@FoxNews', 'at', '10:00', 'AM.', 'Talking', 'about', 'the', 'Southern', 'Border', 'and', 'how']\n"
     ]
    }
   ],
   "source": [
    "tweet_texts = ' '.join(tweets['text'])\n",
    "\n",
    "tokens = tweet_texts.split();\n",
    "\n",
    "token_count = Counter(tokens)\n",
    "\n",
    "print(token_count.most_common(20))\n",
    "\n",
    "print(tokens[0:17])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welche Nachbarbeitungsschritte könnten im Hinblick auf die spätere Verarbeitung einzelner Zeichen sinnvoll sein?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "\n",
    "cleaned_tweet_texts  = html.unescape(tweet_texts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exkursion: SpaCy\n",
    "\n",
    "[```spaCy```](https://spacy.io/) ist eine recht verbreitete Bibliothek im Bereich NLP. Um damit erste Erfahrungen zu sammeln, tokenisieren wir unsere Texte unter Verwendung des ```nlp```-Objekts erneut und filtern alle Satzzeichen und Stopwörter aus. Aus Performancegründen verarbeiten wir die Texte nacheinander und nicht den konkatenierten Gesamttext.\n",
    "\n",
    "Was fällt bei der Betrachtung der Top Tokens auf?\n",
    "\n",
    "**Hinweise**\n",
    "1. Benutzung von ```spaCy```:\n",
    "    * Mittels des ```nlp```-Objekts lassen sich aus Strings Dokumente erzeugen, die Tokens beinhalten.\n",
    "    * ```spaCy``` versieht die Tokens mit zusätzlichen Informationen zum Beispiel darüber, ob es sich bei dem Token um ein Stopwort oder ein Satzzeichen handelt (https://spacy.io/api/token). Wenn wir nur am Text interessiert sind, können wir auf diesen mittels ```token.text``` zugreifen. Ob es sich um ein Stopwort handelt, verrät ```token.is_stop```, ```token.is_punct``` gibt zurück, ob es sich bei dem entsprechenden Token um ein Satzzeichen handelt.\n",
    "2. Aufwand: Beim Betrachten der Top 20 Tokens sollte deutlich werden, dass auch der Rückgriff auf \"Out-of-the-box\"-Lösungen nicht heißt, dass Nachdenken unnötig wird. Wir wollen das aber an dieser Stelle nicht weiter vertiefen, weil wir ohnehin auf Zeichenebene arbeiten wollen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en', disable=['ner', 'tagger', 'parser', 'textcat'])\n",
    "tweet_list = list(tweets['text'])\n",
    "nlp_docs = # TODO Liste von spaCy docs aus Liste der Tweets erstellen\n",
    "nlp_annotated_tokens = # TODO Liste von spaCy tokens aus Liste der Dokumente erstellen\n",
    "nlp_tokens = # TODO (gefilterte) Liste von Strings erstellen\n",
    "# TODO Top 20 Terme ausgeben"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Verwendete Zeichen\n",
    "Zum Ende unserer Analysephase schauen wir uns noch an, welche Einzelzeichen in den Tweets vorkommen. Dazu greifen wir wieder auf die konkatenierten (und bereinigten) Tweettexte zurück, die in der Variable ```cleaned_tweet_texts``` gespeichert sind.\n",
    "Wie viele Zeichen gibt es und wie häufig werden sie verwendet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste aller verwendeten Zeichen inklusive Duplikate (String => Liste)\n",
    "tweet_chars = list(cleaned_tweet_texts)\n",
    "# Wie oft wird welches Zeichen verwendet?\n",
    "cleaned_tweet_stats = Counter(tweet_chars)\n",
    "# Welche Zeichen kommen vor?\n",
    "cleaned_tweet_chars = cleaned_tweet_stats.keys()\n",
    "\n",
    "least_common_chars_touple = cleaned_tweet_stats.most_common()[-107:-1]\n",
    "\n",
    "least_common_chars = list(np.array(least_common_chars_touple)[:,0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Vom Text zur Zeichenliste\n",
    "Betrachten Sie die in der vorherigen Teilaufgabe generierte Liste der verwendeten Zeichen. Welche Bereinigungsschritte könnten angemessen sein? Kurze Begründung bitte.\n",
    "\n",
    "Bevor wir daran gehen, vektorisierte Trainingsdaten für unser Modell zu generieren, wandeln wir unsere Tweets in dieser Teilaufgabe in eine (bereinigte) Liste von Einzelzeichen um und speichern diese in der Variable ```cleaned_tweet_chars```.\n",
    "\n",
    "```['d', 'i', 'e', 's', ' ', 'i', 's', 't', ' ', 'e', 'i', 'n', ' ', 'b', 'e', 'i', 's', 'p', 'i', 'e', 'l', ',', ' ', 'w', 'i', 'e', ' ', 'd', 'i', 'e', ' ', 'l', 'i', 's', 't', 'e', ' ', 'b', 'e', 'g', 'i', 'n', 'n', 'e', 'n', ' ', 'k', 'ö', 'n', 'n', 't', 'e', '.']```\n",
    "\n",
    "\n",
    "**Hinweise**: \n",
    "* In Python gibt es eine Methode ```str.isprintable()```, die bei der Bereinigung hilfreich sein könnte ...\n",
    "* Vereinfachung ist legitim. Je mehr Einzelzeichen wir bei der Modellierung berücksichtigen, umso komplexer wird unser Modell und umso höher ist der Trainingsaufwand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['W',\n",
       " 'i',\n",
       " 'l',\n",
       " 'l',\n",
       " ' ',\n",
       " 'b',\n",
       " 'e',\n",
       " ' ',\n",
       " 'i',\n",
       " 'n',\n",
       " 't',\n",
       " 'e',\n",
       " 'r',\n",
       " 'v',\n",
       " 'i',\n",
       " 'e',\n",
       " 'w',\n",
       " 'e',\n",
       " 'd',\n",
       " ' ',\n",
       " 'b',\n",
       " 'y',\n",
       " ' ',\n",
       " '@',\n",
       " 'M',\n",
       " 'a',\n",
       " 'r',\n",
       " 'i',\n",
       " 'a',\n",
       " 'B',\n",
       " 'a',\n",
       " 'r',\n",
       " 't',\n",
       " 'i',\n",
       " 'r',\n",
       " 'o',\n",
       " 'm',\n",
       " 'o',\n",
       " ' ',\n",
       " 'o',\n",
       " 'n',\n",
       " ' ',\n",
       " '@',\n",
       " 'F',\n",
       " 'o',\n",
       " 'x',\n",
       " 'N',\n",
       " 'e',\n",
       " 'w',\n",
       " 's',\n",
       " ' ',\n",
       " 'a',\n",
       " 't',\n",
       " ' ',\n",
       " '1',\n",
       " '0',\n",
       " ':',\n",
       " '0',\n",
       " '0',\n",
       " ' ',\n",
       " 'A',\n",
       " 'M',\n",
       " '.',\n",
       " ' ',\n",
       " 'T',\n",
       " 'a',\n",
       " 'l',\n",
       " 'k',\n",
       " 'i',\n",
       " 'n',\n",
       " 'g',\n",
       " ' ',\n",
       " 'a',\n",
       " 'b',\n",
       " 'o',\n",
       " 'u',\n",
       " 't',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " ' ',\n",
       " 'S',\n",
       " 'o',\n",
       " 'u',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " 'r',\n",
       " 'n',\n",
       " ' ',\n",
       " 'B',\n",
       " 'o',\n",
       " 'r',\n",
       " 'd',\n",
       " 'e',\n",
       " 'r',\n",
       " ' ',\n",
       " 'a',\n",
       " 'n',\n",
       " 'd',\n",
       " ' ',\n",
       " 'h',\n",
       " 'o',\n",
       " 'w',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " ' ',\n",
       " 'D',\n",
       " 'e',\n",
       " 'm',\n",
       " 's',\n",
       " ' ',\n",
       " 'M',\n",
       " 'U',\n",
       " 'S',\n",
       " 'T',\n",
       " ' ',\n",
       " 'a',\n",
       " 'c',\n",
       " 't',\n",
       " ' ',\n",
       " 'f',\n",
       " 'a',\n",
       " 's',\n",
       " 't',\n",
       " ' ',\n",
       " 't',\n",
       " 'o',\n",
       " ' ',\n",
       " 'c',\n",
       " 'h',\n",
       " 'a',\n",
       " 'n',\n",
       " 'g',\n",
       " 'e',\n",
       " ' ',\n",
       " 'o',\n",
       " 'u',\n",
       " 'r',\n",
       " ' ',\n",
       " 'p',\n",
       " 'a',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " 't',\n",
       " 'i',\n",
       " 'c',\n",
       " ' ',\n",
       " 'i',\n",
       " 'm',\n",
       " 'm',\n",
       " 'i',\n",
       " 'g',\n",
       " 'r',\n",
       " 'a',\n",
       " 't',\n",
       " 'i',\n",
       " 'o',\n",
       " 'n',\n",
       " ' ',\n",
       " 'l',\n",
       " 'a',\n",
       " 'w',\n",
       " 's',\n",
       " '.',\n",
       " ' ',\n",
       " 'W',\n",
       " 'i',\n",
       " 'l',\n",
       " 'l',\n",
       " ' ',\n",
       " 'b',\n",
       " 'e',\n",
       " ' ',\n",
       " 't',\n",
       " 'o',\n",
       " 'u',\n",
       " 'g',\n",
       " 'h',\n",
       " ',',\n",
       " ' ',\n",
       " 'w',\n",
       " 'a',\n",
       " 't',\n",
       " 'c',\n",
       " 'h',\n",
       " '!',\n",
       " ' ',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " 'f',\n",
       " 'o',\n",
       " 'r',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " ' ',\n",
       " 'm',\n",
       " 'o',\n",
       " 'r',\n",
       " 'e',\n",
       " ' ',\n",
       " 't',\n",
       " 'r',\n",
       " 'a',\n",
       " 'd',\n",
       " 'i',\n",
       " 't',\n",
       " 'i',\n",
       " 'o',\n",
       " 'n',\n",
       " 'a',\n",
       " 'l',\n",
       " ',',\n",
       " ' ',\n",
       " 'b',\n",
       " 'u',\n",
       " 't',\n",
       " ' ',\n",
       " 'n',\n",
       " 'o',\n",
       " 't',\n",
       " ' ',\n",
       " 'v',\n",
       " 'e',\n",
       " 'r',\n",
       " 'y',\n",
       " ' ',\n",
       " 'b',\n",
       " 'r',\n",
       " 'i',\n",
       " 'g',\n",
       " 'h',\n",
       " 't',\n",
       " ',',\n",
       " ' ',\n",
       " 'S',\n",
       " 'l',\n",
       " 'e',\n",
       " 'e',\n",
       " 'p',\n",
       " 'y',\n",
       " ' ',\n",
       " 'J',\n",
       " 'o',\n",
       " 'e',\n",
       " ' ',\n",
       " 'B',\n",
       " 'i',\n",
       " 'd',\n",
       " 'e',\n",
       " 'n',\n",
       " '.',\n",
       " ' ',\n",
       " 'H',\n",
       " 'e',\n",
       " 'r',\n",
       " 'e',\n",
       " ' ',\n",
       " 'w',\n",
       " 'e',\n",
       " ' ',\n",
       " 'g',\n",
       " 'o',\n",
       " ' ',\n",
       " 'a',\n",
       " 'g',\n",
       " 'a',\n",
       " 'i',\n",
       " 'n',\n",
       " ' ',\n",
       " 'B',\n",
       " 'e',\n",
       " 'r',\n",
       " 'n',\n",
       " 'i',\n",
       " 'e',\n",
       " ',',\n",
       " ' ',\n",
       " 'b',\n",
       " 'u',\n",
       " 't',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'i',\n",
       " 's',\n",
       " ' ',\n",
       " 't',\n",
       " 'i',\n",
       " 'm',\n",
       " 'e',\n",
       " ' ',\n",
       " 'p',\n",
       " 'l',\n",
       " 'e',\n",
       " 'a',\n",
       " 's',\n",
       " 'e',\n",
       " ' ',\n",
       " 's',\n",
       " 'h',\n",
       " 'o',\n",
       " 'w',\n",
       " ' ',\n",
       " 'a',\n",
       " ' ',\n",
       " 'l',\n",
       " 'i',\n",
       " 't',\n",
       " 't',\n",
       " 'l',\n",
       " 'e',\n",
       " ' ',\n",
       " 'm',\n",
       " 'o',\n",
       " 'r',\n",
       " 'e',\n",
       " ' ',\n",
       " 'a',\n",
       " 'n',\n",
       " 'g',\n",
       " 'e',\n",
       " 'r',\n",
       " ' ',\n",
       " 'a',\n",
       " 'n',\n",
       " 'd',\n",
       " ' ',\n",
       " 'i',\n",
       " 'n',\n",
       " 'd',\n",
       " 'i',\n",
       " 'g',\n",
       " 'n',\n",
       " 'a',\n",
       " 't',\n",
       " 'i',\n",
       " 'o',\n",
       " 'n',\n",
       " ' ',\n",
       " 'w',\n",
       " 'h',\n",
       " 'e',\n",
       " 'n',\n",
       " ' ',\n",
       " 'y',\n",
       " 'o',\n",
       " 'u',\n",
       " ' ',\n",
       " 'g',\n",
       " 'e',\n",
       " 't',\n",
       " ' ',\n",
       " 's',\n",
       " 'c',\n",
       " 'r',\n",
       " 'e',\n",
       " 'w',\n",
       " 'e',\n",
       " 'd',\n",
       " '!',\n",
       " ' ',\n",
       " 'T',\n",
       " 'h',\n",
       " 'e',\n",
       " ' ',\n",
       " 'D',\n",
       " 'e',\n",
       " 'm',\n",
       " 'o',\n",
       " 'c',\n",
       " 'r',\n",
       " 'a',\n",
       " 't',\n",
       " 'i',\n",
       " 'c',\n",
       " ' ',\n",
       " 'N',\n",
       " 'a',\n",
       " 't',\n",
       " 'i',\n",
       " 'o',\n",
       " 'n',\n",
       " 'a',\n",
       " 'l',\n",
       " ' ',\n",
       " 'C',\n",
       " 'o',\n",
       " 'm',\n",
       " 'm',\n",
       " 'i',\n",
       " 't',\n",
       " 't',\n",
       " 'e',\n",
       " 'e',\n",
       " ',',\n",
       " ' ',\n",
       " 's',\n",
       " 'o',\n",
       " 'm',\n",
       " 'e',\n",
       " 't',\n",
       " 'i',\n",
       " 'm',\n",
       " 'e',\n",
       " 's',\n",
       " ' ',\n",
       " 'r',\n",
       " 'e',\n",
       " 'f',\n",
       " 'e',\n",
       " 'r',\n",
       " 'r',\n",
       " 'e',\n",
       " 'd',\n",
       " ' ',\n",
       " 't',\n",
       " 'o',\n",
       " ' ',\n",
       " 'a',\n",
       " 's',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " ' ',\n",
       " 'D',\n",
       " 'N',\n",
       " 'C',\n",
       " ',',\n",
       " ' ',\n",
       " 'i',\n",
       " 's',\n",
       " ' ',\n",
       " 'a',\n",
       " 'g',\n",
       " 'a',\n",
       " 'i',\n",
       " 'n',\n",
       " ' ',\n",
       " 'w',\n",
       " 'o',\n",
       " 'r',\n",
       " 'k',\n",
       " 'i',\n",
       " 'n',\n",
       " 'g',\n",
       " ' ',\n",
       " 'i',\n",
       " 't',\n",
       " 's',\n",
       " ' ',\n",
       " 'm',\n",
       " 'a',\n",
       " 'g',\n",
       " 'i',\n",
       " 'c',\n",
       " ' ',\n",
       " 'i',\n",
       " 'n',\n",
       " ' ',\n",
       " 'i',\n",
       " 't',\n",
       " 's',\n",
       " ' ',\n",
       " 'q',\n",
       " 'u',\n",
       " 'e',\n",
       " 's',\n",
       " 't',\n",
       " ' ',\n",
       " 't',\n",
       " 'o',\n",
       " ' ',\n",
       " 'd',\n",
       " 'e',\n",
       " 's',\n",
       " 't',\n",
       " 'r',\n",
       " 'o',\n",
       " 'y',\n",
       " ' ',\n",
       " 'C',\n",
       " 'r',\n",
       " 'a',\n",
       " 'z',\n",
       " 'y',\n",
       " ' ',\n",
       " 'B',\n",
       " 'e',\n",
       " 'r',\n",
       " 'n',\n",
       " 'i',\n",
       " 'e',\n",
       " ' ',\n",
       " 'S',\n",
       " 'a',\n",
       " 'n',\n",
       " 'd',\n",
       " 'e',\n",
       " 'r',\n",
       " 's',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " ' ',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " 'E',\n",
       " 'v',\n",
       " 'e',\n",
       " 'r',\n",
       " ' ',\n",
       " 's',\n",
       " 'i',\n",
       " 'n',\n",
       " 'c',\n",
       " 'e',\n",
       " ' ',\n",
       " 'A',\n",
       " 'n',\n",
       " 'd',\n",
       " 'r',\n",
       " 'e',\n",
       " 'w',\n",
       " ' ',\n",
       " 'c',\n",
       " 'a',\n",
       " 'm',\n",
       " 'e',\n",
       " ' ',\n",
       " 't',\n",
       " 'o',\n",
       " ' ',\n",
       " 'm',\n",
       " 'y',\n",
       " ' ',\n",
       " 'o',\n",
       " 'f',\n",
       " 'f',\n",
       " 'i',\n",
       " 'c',\n",
       " 'e',\n",
       " ' ',\n",
       " 't',\n",
       " 'o',\n",
       " ' ',\n",
       " 'a',\n",
       " 's',\n",
       " 'k',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'a',\n",
       " 't',\n",
       " ' ',\n",
       " 'I',\n",
       " ' ',\n",
       " 'a',\n",
       " 'p',\n",
       " 'p',\n",
       " 'o',\n",
       " 'i',\n",
       " 'n',\n",
       " 't',\n",
       " ' ',\n",
       " 'h',\n",
       " 'i',\n",
       " 'm',\n",
       " ' ',\n",
       " 't',\n",
       " 'o',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " ' ',\n",
       " 'U',\n",
       " '.',\n",
       " 'S',\n",
       " '.',\n",
       " ' ',\n",
       " 'S',\n",
       " 'u',\n",
       " 'p',\n",
       " 'r',\n",
       " 'e',\n",
       " 'm',\n",
       " 'e',\n",
       " ' ',\n",
       " 'C',\n",
       " 'o',\n",
       " 'u',\n",
       " 'r',\n",
       " 't',\n",
       " ',',\n",
       " ' ',\n",
       " 'a',\n",
       " 'n',\n",
       " 'd',\n",
       " ' ',\n",
       " 'I',\n",
       " ' ',\n",
       " 's',\n",
       " 'a',\n",
       " 'i',\n",
       " 'd',\n",
       " ' ',\n",
       " 'N',\n",
       " 'O',\n",
       " ',',\n",
       " ' ',\n",
       " 'h',\n",
       " 'e',\n",
       " ' ',\n",
       " 'h',\n",
       " 'a',\n",
       " 's',\n",
       " ' ',\n",
       " 'b',\n",
       " 'e',\n",
       " 'e',\n",
       " 'n',\n",
       " ' ',\n",
       " 'v',\n",
       " 'e',\n",
       " 'r',\n",
       " 'y',\n",
       " ' ',\n",
       " 'h',\n",
       " 'o',\n",
       " 's',\n",
       " 't',\n",
       " 'i',\n",
       " 'l',\n",
       " 'e',\n",
       " '!',\n",
       " ' ',\n",
       " 'A',\n",
       " 'l',\n",
       " 's',\n",
       " 'o',\n",
       " ' ',\n",
       " 'a',\n",
       " 's',\n",
       " 'k',\n",
       " 'e',\n",
       " 'd',\n",
       " ' ',\n",
       " 'f',\n",
       " 'o',\n",
       " 'r',\n",
       " ' ',\n",
       " 'p',\n",
       " 'a',\n",
       " 'r',\n",
       " 'd',\n",
       " 'o',\n",
       " 'n',\n",
       " ' ',\n",
       " 'f',\n",
       " 'o',\n",
       " 'r',\n",
       " ' ',\n",
       " 'h',\n",
       " 'i',\n",
       " 's',\n",
       " ' ',\n",
       " 'f',\n",
       " 'r',\n",
       " 'i',\n",
       " 'e',\n",
       " 'n',\n",
       " 'd',\n",
       " '.',\n",
       " ' ',\n",
       " 'A',\n",
       " ' ',\n",
       " 'g',\n",
       " 'o',\n",
       " 'o',\n",
       " 'd',\n",
       " ' ',\n",
       " '“',\n",
       " 'p',\n",
       " 'a',\n",
       " 'l',\n",
       " '”',\n",
       " ' ',\n",
       " 'o',\n",
       " 'f',\n",
       " ' ',\n",
       " 'l',\n",
       " 'o',\n",
       " 'w',\n",
       " ' ',\n",
       " 'r',\n",
       " 'a',\n",
       " 't',\n",
       " 'i',\n",
       " 'n',\n",
       " 'g',\n",
       " 's',\n",
       " ' ',\n",
       " 'S',\n",
       " 'h',\n",
       " 'e',\n",
       " 'p',\n",
       " 'a',\n",
       " 'r',\n",
       " 'd',\n",
       " ' ',\n",
       " 'S',\n",
       " 'm',\n",
       " 'i',\n",
       " 't',\n",
       " 'h',\n",
       " '.',\n",
       " ' ',\n",
       " 'T',\n",
       " 'h',\n",
       " 'a',\n",
       " 'n',\n",
       " 'k',\n",
       " ' ',\n",
       " 'y',\n",
       " 'o',\n",
       " 'u',\n",
       " ' ',\n",
       " 't',\n",
       " 'o',\n",
       " ' ',\n",
       " 'b',\n",
       " 'r',\n",
       " 'i',\n",
       " 'l',\n",
       " 'l',\n",
       " 'i',\n",
       " 'a',\n",
       " 'n',\n",
       " 't',\n",
       " ' ',\n",
       " 'a',\n",
       " 'n',\n",
       " 'd',\n",
       " ' ',\n",
       " 'h',\n",
       " 'i',\n",
       " 'g',\n",
       " 'h',\n",
       " 'l',\n",
       " 'y',\n",
       " ' ',\n",
       " 'r',\n",
       " 'e',\n",
       " 's',\n",
       " 'p',\n",
       " 'e',\n",
       " 'c',\n",
       " 't',\n",
       " 'e',\n",
       " 'd',\n",
       " ' ',\n",
       " 'a',\n",
       " 't',\n",
       " 't',\n",
       " 'o',\n",
       " 'r',\n",
       " 'n',\n",
       " 'e',\n",
       " 'y',\n",
       " ' ',\n",
       " 'A',\n",
       " 'l',\n",
       " 'a',\n",
       " 'n',\n",
       " ' ',\n",
       " 'D',\n",
       " 'e',\n",
       " 'r',\n",
       " 's',\n",
       " 'h',\n",
       " 'o',\n",
       " 'w',\n",
       " 'i',\n",
       " 't',\n",
       " 'z',\n",
       " ' ',\n",
       " 'f',\n",
       " 'o',\n",
       " 'r',\n",
       " ' ',\n",
       " 'd',\n",
       " 'e',\n",
       " 's',\n",
       " 't',\n",
       " 'r',\n",
       " 'o',\n",
       " 'y',\n",
       " 'i',\n",
       " 'n',\n",
       " 'g',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " ' ',\n",
       " 'v',\n",
       " 'e',\n",
       " 'r',\n",
       " 'y',\n",
       " ' ',\n",
       " 'd',\n",
       " 'u',\n",
       " 'm',\n",
       " 'b',\n",
       " ' ',\n",
       " 'l',\n",
       " 'e',\n",
       " 'g',\n",
       " 'a',\n",
       " 'l',\n",
       " ' ',\n",
       " 'a',\n",
       " 'r',\n",
       " 'g',\n",
       " 'u',\n",
       " 'm',\n",
       " 'e',\n",
       " 'n',\n",
       " 't',\n",
       " ' ',\n",
       " 'o',\n",
       " 'f',\n",
       " ' ',\n",
       " '“',\n",
       " 'J',\n",
       " 'u',\n",
       " 'd',\n",
       " 'g',\n",
       " 'e',\n",
       " '”',\n",
       " ' ',\n",
       " 'A',\n",
       " 'n',\n",
       " 'd',\n",
       " 'r',\n",
       " 'e',\n",
       " 'w',\n",
       " ' ',\n",
       " 'N',\n",
       " 'a',\n",
       " 'p',\n",
       " 'o',\n",
       " 'l',\n",
       " 'i',\n",
       " 't',\n",
       " 'a',\n",
       " 'n',\n",
       " 'o',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " ' ',\n",
       " 'T',\n",
       " 'h',\n",
       " 'a',\n",
       " 'n',\n",
       " 'k',\n",
       " ' ',\n",
       " 'y',\n",
       " 'o',\n",
       " 'u',\n",
       " ' ',\n",
       " 'G',\n",
       " 'r',\n",
       " 'e',\n",
       " 'e',\n",
       " 'n',\n",
       " ' ',\n",
       " 'B',\n",
       " 'a',\n",
       " 'y',\n",
       " ',',\n",
       " ' ',\n",
       " 'W',\n",
       " 'i',\n",
       " 's',\n",
       " 'c',\n",
       " 'o',\n",
       " 'n',\n",
       " 's',\n",
       " 'i',\n",
       " 'n',\n",
       " '!',\n",
       " ' ',\n",
       " 'M',\n",
       " 'A',\n",
       " 'K',\n",
       " 'E',\n",
       " ' ',\n",
       " 'A',\n",
       " 'M',\n",
       " 'E',\n",
       " 'R',\n",
       " 'I',\n",
       " 'C',\n",
       " 'A',\n",
       " ' ',\n",
       " 'G',\n",
       " 'R',\n",
       " 'E',\n",
       " 'A',\n",
       " 'T',\n",
       " ' ',\n",
       " 'A',\n",
       " 'G',\n",
       " 'A',\n",
       " 'I',\n",
       " 'N',\n",
       " '!',\n",
       " '!',\n",
       " ' ',\n",
       " 'h',\n",
       " 't',\n",
       " 't',\n",
       " 'p',\n",
       " 's',\n",
       " ':',\n",
       " '/',\n",
       " '/',\n",
       " 't',\n",
       " '.',\n",
       " 'c',\n",
       " 'o',\n",
       " '/',\n",
       " 'c',\n",
       " 'h',\n",
       " 'G',\n",
       " 'L',\n",
       " 'x',\n",
       " 'G',\n",
       " 's',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'h',\n",
       " ' ',\n",
       " 'B',\n",
       " 'e',\n",
       " 'a',\n",
       " 'u',\n",
       " 't',\n",
       " 'i',\n",
       " 'f',\n",
       " 'u',\n",
       " 'l',\n",
       " ' ',\n",
       " '#',\n",
       " 'T',\n",
       " 'r',\n",
       " 'u',\n",
       " 'm',\n",
       " 'p',\n",
       " 'R',\n",
       " 'a',\n",
       " 'l',\n",
       " 'l',\n",
       " 'y',\n",
       " ' ',\n",
       " 't',\n",
       " 'o',\n",
       " 'n',\n",
       " 'i',\n",
       " 'g',\n",
       " 'h',\n",
       " 't',\n",
       " ' ',\n",
       " 'i',\n",
       " 'n',\n",
       " ' ',\n",
       " 'G',\n",
       " 'r',\n",
       " 'e',\n",
       " 'e',\n",
       " 'n',\n",
       " ' ',\n",
       " 'B',\n",
       " 'a',\n",
       " ...]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "black_list = least_common_chars\n",
    "# Alle Tweets in Einzelzeichen zerlegt und ggf. bereinigt hier rein\n",
    "cleaned_tweet_chars = list(filter(lambda x: x not in black_list, tweet_chars))\n",
    "cleaned_tweet_chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 2\n",
    "Nachdem wir uns einen Überblick über den Datensatz verschafft und uns für die Zeichen entschieden haben, die wir bei der Modellierung berücksichtigen wollen, geht es nun darum, die Daten so aufzubereiten, dass wir ein Modell trainieren können.\n",
    "\n",
    "Das Training soll wie folgt ablaufen: Gegeben 30 Zeichen, soll das Modell das 31. Zeichen vorhersagen. Dazu müssen wir die einzelnen Zeichen in Vektorform bringen. Wir wählen dazu ein One-Hot-Encoding und bilden folglich jedes der Einzelzeichen in ```cleaned_tweet_chars``` auf einen Vektor ab, der genau eine 1 enthält.\n",
    "\n",
    "(Zu) einfaches Beispiel: ```['a', 'b', 'c'] => [(1,0,0), (0,1,0), (0,0,1)]```\n",
    "\n",
    "### 2.1 Indizes für das One-Hot-Encoding\n",
    "Um entscheiden zu können, an welcher Stelle wir die 1 setzen, müssen wir jedem Zeichen einen eindeutigen Index zuweisen.\n",
    "Umgekehrt wollen wir auch zu jedem Index schnell das zugehörige Zeichen ermitteln können. Wir erstellen daher zwei Dictionaries: ```char2index``` für die Abbildung von Zeichen zu Index und ```index2char``` für die umgekehrte Abbildung von Index zu Zeichen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl Zeichen: 89\n"
     ]
    }
   ],
   "source": [
    "# Menge aller verwendeten Zeichen nach Bereinigung \n",
    "char_set = set(cleaned_tweet_chars)\n",
    "print('Anzahl Zeichen: {}'.format(len(char_set)))\n",
    "char2index = {}\n",
    "index2char = {}\n",
    "for index, char in enumerate(char_set):\n",
    "    char2index[char] = index\n",
    "    index2char[index] = char\n",
    "# index2char = # TODO Abbildung Index => Zeichen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Generiere Trainingsdaten\n",
    "Wie bereits beschrieben, soll das Modell 30 Zeichen entgegennehmen und das 31. Zeichen vorhersagen. Wir generieren uns Trainingsdaten, indem wir ```sentences``` eine Liste von 30 Zeichen aus ```cleaned_tweet_chars``` hinzufügen, ```next_chars``` das darauf folgende 31. Zeichen und dies alle 4 Zeichen wiederholen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl Trainingssätze: 779415\n"
     ]
    }
   ],
   "source": [
    "input_length = 30\n",
    "step = 4\n",
    "sentences = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(cleaned_tweet_chars)-input_length-1, step):\n",
    "    sentences.append(cleaned_tweet_chars[i:min(i+input_length, len(cleaned_tweet_chars)-1)])\n",
    "    next_chars.append(cleaned_tweet_chars[min(i+input_length+1, len(cleaned_tweet_chars)-1)])\n",
    "print('Anzahl Trainingssätze: {}'.format(len(sentences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Vektorisierung\n",
    "Nun können wir daran gehen, die Daten zu vektorisieren. Die Eingabe ```x``` enthält für jeden der Trainingssätze in ```sentences``` one-hot-codierte Vektoren der Zeichen, aus denen der jeweilige Satz besteht. \n",
    "Die erwartete Ausgabe ```y``` basiert auf ```next_chars``` und enthält für jeden der Trainingssätze einen einzelnen one-hot-codierten Vektor für das als Fortsetzung des Satzes erwartete Zeichen.\n",
    "\n",
    "Beispiel: Wenn ```sentences``` an Position ```i``` die Sequenz ```['a', 'b', 'c']``` und ```next_chars``` an derselben Stelle ```'d'``` enthält, dann enthält ```x``` bei entsprechender Kodierung an Position ```i``` ```[[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0]]``` und ```y``` an der entsprechenden Stelle ```[0, 0, 0, 1]```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vektorisierung ...\n",
      "[[[False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  ...\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]]\n",
      "\n",
      " [[False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  ...\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]]\n",
      "\n",
      " [[False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  ...\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  ...\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]]\n",
      "\n",
      " [[False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  ...\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]]\n",
      "\n",
      " [[False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  ...\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]]]\n",
      "[[False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " ...\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print('Vektorisierung ...')\n",
    "x = np.zeros((len(sentences), input_length, len(char_set)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(char_set)), dtype=np.bool)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i][t][char2index[char]] = 1\n",
    "    y[i][char2index[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " ...\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]]\n",
      "[False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False  True False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False]\n"
     ]
    }
   ],
   "source": [
    "print(x[0])\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Definition des Modells\n",
    "\n",
    "Für unseren Tweet-Generator werden wir ein recht einfaches Modell trainieren. Wir verwenden wieder die Sequential-API. \n",
    "\n",
    "Der erste Layer ist direkt das Herzstück unseres Modells: Der LSTM-Layer. \n",
    "\n",
    "Als Anzahl der Units verwenden wir 128.\n",
    "Welche Dimensionen hat die ```input_shape```? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erstelle Model...\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import metrics\n",
    "\n",
    "print('Erstelle Model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(input_length, len(char_set))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als Ausgabelayer fügen wir einen Dense-Layer hinzu. Wie müssen wir die Anzahl der Hidden-Units wählen? Wieso ist ```softmax``` eine geeignete Aktivierungsfunktion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(len(char_set), activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als Optimizer wählen wir RMSprop mit einer Learning-Rate von 0.005 und als Loss-Funktion ```categorical_crossentropy```.\n",
    "\n",
    "Wieso können wir nicht wie im letzten Labor ```binary_crossentropy``` verwenden?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 128)               111616    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 89)                11481     \n",
      "=================================================================\n",
      "Total params: 123,097\n",
      "Trainable params: 123,097\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimizer = RMSprop(lr=0.005)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Zusammenhang mit LSTMs werden häufig Dropout-Layer erwähnt. Bei Betrachtung der Zusammenfassung unseres Modells fällt auf, dass wir keinen solchen Layer verwenden.\n",
    "\n",
    "Wozu dienen Dropout-Layer? Warum könnte es in unserem Fall angemessen sein, auf einen solchen Layer zu verzichten?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Training des Models\n",
    "\n",
    "Nach all den Vorarbeiten können wir nun daran gehen, unser Model zu trainieren.\n",
    "Um das Model bei Bedarf nicht neu definieren zu müssen, speichern wir uns dessen Struktur als JSON ab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_structure = model.to_json()\n",
    "with open(\"text_generation_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um unsere Trainingsfortschritte nicht zu verlieren, definieren wir uns eine Checkpoint-Funktion, die als Callback aufgerufen wird und den aktuellen Modelzustand abspeichert, solange das Modell besser ist, als das bisher gespeicherte Model. Gespeichert werden soll das komplette Model, nicht nur die Gewichte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "model_checkpoint = ModelCheckpoint('text_generation.hd5', monitor='acc', save_best_only=True, save_weights_only=False)# TODO nur bestes Model speichern, komplettes Model speichern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt wird es ernst. Wir trainieren unser Model. Um in ansehbarer Zeit Ergebnisse zu sehen, führen wir unser Training über 20 Epochen mit eine Batch-Size von 100 durch.\n",
    "\n",
    "Um die Entwicklung des Models später auswerten zu können, speichern wir uns die History."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/20\n",
      "779415/779415 [==============================] - 425s 545us/step - loss: 2.5822 - acc: 0.3285\n",
      "Epoch 2/20\n",
      "779415/779415 [==============================] - 426s 546us/step - loss: 2.3356 - acc: 0.3895\n",
      "Epoch 3/20\n",
      "779415/779415 [==============================] - 424s 543us/step - loss: 2.2735 - acc: 0.4052\n",
      "Epoch 4/20\n",
      "779415/779415 [==============================] - 422s 542us/step - loss: 2.2422 - acc: 0.4128\n",
      "Epoch 5/20\n",
      "779415/779415 [==============================] - 422s 542us/step - loss: 2.2227 - acc: 0.4178\n",
      "Epoch 6/20\n",
      "779415/779415 [==============================] - 421s 540us/step - loss: 2.2099 - acc: 0.4210\n",
      "Epoch 7/20\n",
      "779415/779415 [==============================] - 421s 540us/step - loss: 2.2005 - acc: 0.4235\n",
      "Epoch 8/20\n",
      "779415/779415 [==============================] - 420s 539us/step - loss: 2.1927 - acc: 0.4260\n",
      "Epoch 9/20\n",
      "779415/779415 [==============================] - 420s 539us/step - loss: 2.1882 - acc: 0.4274\n",
      "Epoch 10/20\n",
      "779415/779415 [==============================] - 420s 539us/step - loss: 2.1830 - acc: 0.4291\n",
      "Epoch 11/20\n",
      "779415/779415 [==============================] - 417s 535us/step - loss: 2.1803 - acc: 0.4293\n",
      "Epoch 12/20\n",
      "779415/779415 [==============================] - 419s 538us/step - loss: 2.1782 - acc: 0.4308\n",
      "Epoch 13/20\n",
      "779415/779415 [==============================] - 418s 537us/step - loss: 2.1758 - acc: 0.4317\n",
      "Epoch 14/20\n",
      "779415/779415 [==============================] - 418s 536us/step - loss: 2.1733 - acc: 0.4325\n",
      "Epoch 15/20\n",
      "779415/779415 [==============================] - 418s 536us/step - loss: 2.1714 - acc: 0.4328\n",
      "Epoch 16/20\n",
      "779415/779415 [==============================] - 417s 535us/step - loss: 2.1711 - acc: 0.4333\n",
      "Epoch 17/20\n",
      "779415/779415 [==============================] - 419s 538us/step - loss: 2.1706 - acc: 0.4339\n",
      "Epoch 18/20\n",
      "779415/779415 [==============================] - 419s 537us/step - loss: 2.1700 - acc: 0.4336\n",
      "Epoch 19/20\n",
      "779415/779415 [==============================] - 417s 535us/step - loss: 2.1750 - acc: 0.4347\n",
      "Epoch 20/20\n",
      "779415/779415 [==============================] - 418s 536us/step - loss: 2.1779 - acc: 0.4343\n",
      "History gespeichert.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 100\n",
    "\n",
    "\n",
    "model_history = model.fit(x, y, batch_size=batch_size, epochs=epochs, callbacks=[model_checkpoint])\n",
    "\n",
    "with open(\"text_generation_history\", 'wb') as hist_file:\n",
    "    pickle.dump(model_history.history, hist_file)\n",
    "print('History gespeichert.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 3\n",
    "Zum Abschluss wollen wir noch etwas Spaß mit unserem Modell haben. Aus diesem Grund laden wir uns das gespeicherte (bisher) beste Model und verwenden es, um basierend auf einem \"Seed\" neue Tweets zu generieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "loaded_model = load_model('text_generation.hd5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eine kleine Hilfsfunktion\n",
    "Der Ausgabelayer unseres Models beschreibt eine Wahrscheinlichkeitsverteilung über alle möglichen Ausgabezeichen. Was wir tun wollen, ist anhand dieser Verteilung eine repräsentative Stichprobe zu ziehen. Ähnlich wie bei Markov-Ketten wählen wir als nächstes Zeichen nicht zwangsläufig das, mit der höchsten Auftrittswahrscheinlichkeit, denn wir wollen uns ja eine gewissen künstlerisch-kreative Freiheit erhalten, aber wir orientieren uns bei der Auswahl an der Auftrittswahrscheinlichkeit der potentiellen Folgetokens im gegebenen Kontext.\n",
    "\n",
    "Die Hilfsmethode, die wir dazu verwenden, ist aus dem Keras-Tutorial zu LSTMs übernommen. Der Parameter ```temperature``` schärft die ursprüngliche Wahrscheinlichkeitsverteilung oder schwächt sie ab. Bei ```temperature > 1``` ist die Ausgabe diverser, allerdings potentiell auch konfuser, bei ```temperature < 1 ``` bleiben wir näher an den Originaltweets.\n",
    "\n",
    "```multinominal(num_samples, probabilities_list, size``` zieht ```size```-mal ```num_samples``` Beispiele aus einer Verteilung, deren Eigenschaften durch ```probabilities_list``` beschrieben wird. In unserem Fall wollen wir einmal ziehen und zwar genau ein Beispiel. Ausgabe ist demnach eine Liste, die einen einzigen Vektor enthält, der genauso lang ist, wie die Wahrscheinlichkeitsverteilung, die wir in die Funktion hineingeben, und der eine einzige 1 enthält. Für den Index dieser 1 interessieren wir uns, weil wir das entsprechend one-hot-codierte Zeichen als nächstes ausgeben wollen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds.clip(min=0.0001)) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Tweets erzeugen\n",
    "Um neue Tweets zu erzeugen, brauchen wir einen Seed, mit dem unser Model arbeiten kann. Wir machen es uns einfach und wählen einen zufälligen Startindex, ab dem ```input_length``` Zeichen aus unserer langen Tweet-Liste ```cleaned_tweet_chars``` herausgenommen werden.\n",
    "\n",
    "Diese vektorisieren wir dann und füttern unser Model mit dem so entstandenen Vektor, um das nächste Zeichen vorherzusagen, das dann wiederum Teil des Seeds für die nächste Vorhersage ist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "seed_start_index = # TODO Zufallszahl für Startindex innerhalb von cleaned_tweet_chars wählen\n",
    "\n",
    "for diversity in [0.2, 0.5, 0.8, 1.0, 1.2]:\n",
    "    print()\n",
    "    print('----- Diversität:', diversity)\n",
    "\n",
    "    generated = ''\n",
    "    sentence = # Subliste der passenden Länge aus cleaned_tweet_charts extrahieren\n",
    "    generated += ''.join(sentence)\n",
    "    print('----- Erzeuge Tweet aus Seed: \"' + ''.join(sentence) + '\"\\n')\n",
    "    sys.stdout.write(generated)\n",
    "\n",
    "    for i in range(250):\n",
    "        x = np.zeros((1, #TODO 2. Dimension?, len(char_set)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[0, t, # TODO Wo muss die 1 hin?] = 1.\n",
    "\n",
    "        preds = loaded_model.predict(x, verbose=0)[0]\n",
    "        next_index = sample(preds, diversity)\n",
    "        next_char = # TODO Welchem Zeichen entspricht der Index?\n",
    "\n",
    "        generated += next_char\n",
    "        sentence = # TODO sentence bleibt eine Liste von Zeichen, aber das erste fällt weg und next_char kommt am Ende neu hinzu\n",
    "\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Entwicklung von Loss und Accuracy\n",
    "Um die Aufgabe abzuschließen, wollen wir noch einen Blick auf die Entwicklung von Loss und Accuracy über die Trainingsepochen werfen. Wir haben die \"Geschichte\" unseres Models in der Datei ```text_generation_history``` abgespeichert.\n",
    "\n",
    "Nutzt ```pyplot```, um die Entwicklung von Accuracy und Loss grafisch darzustellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open(\"text_generation_history\", 'rb') as hist_file:\n",
    "    history = pickle.load(hist_file)\n",
    "    \n",
    "plt.plot(# Entwicklung der Accuracy)\n",
    "plt.title('Accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()\n",
    "\n",
    "# TODO Loss plotten\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
